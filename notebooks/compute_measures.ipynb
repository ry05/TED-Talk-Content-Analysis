{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd05c826387ab78e5301e950da683ca05cf4901dadac255ab8de11ebb9302ea2318",
   "display_name": "Python 3.8.5 64-bit ('wids': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "5c826387ab78e5301e950da683ca05cf4901dadac255ab8de11ebb9302ea2318"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Analysis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install textstat\n",
    "!pip install lexicalrichness\n",
    "!pip install textblob\n",
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import statistics\n",
    "import textstat\n",
    "from lexicalrichness import LexicalRichness\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plotting pretty figures and avoid blurry images\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "source": [
    "## Load Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ted = pd.read_csv('../data/interim/ted_preprocessed.csv')\n",
    "\n",
    "# filter relevant variables\n",
    "ted = ted[['talk_name', 'event', 'event_cat', 'views', 'duration', 'tags', 'p_year', 'p_month',\n",
    "    'transcript', 'syll', 'words', 'sent']]"
   ]
  },
  {
   "source": [
    "## Compute Measures\n",
    "\n",
    "### Complexity Measures\n",
    "\n",
    "Complexity measures computed for this analysis are\n",
    "- Syntactic complexity measures\n",
    "    - Median number of words per sentence(MWS)\n",
    "    - Flesch-Kincaid grade level readability measure(FKL)\n",
    "- Semantic complexity measures\n",
    "    - Measure of textual lexical diversity (MTLD)\n",
    "\n",
    "### Fluency Measures\n",
    "\n",
    "- Syllables per minute(SPM)\n",
    "\n",
    "### TED-specific Measures\n",
    "\n",
    "- Laughter Frequency (LF): The mean number of seconds between each time the audience laughs \n",
    "- Pronominal Measures (PM): The ratio of the number of times a speaker says “I”, “you” or “we” to the total length of the talk in seconds\n",
    "- Numerical Info Proportion (NIP): The ratio of the number of times a speaker uses a numerical word(relating to a number) to the total length of the talk in seconds\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----complexity measures-----\n",
    "def mws(text):\n",
    "    \"\"\"Compute median number of words per sentence\"\"\"\n",
    "\n",
    "    sentences = re.split(r' *[\\.\\?!][\\'\"\\)\\]]*[ |\\n](?=[A-Z])', text)\n",
    "    word_cnt = [textstat.lexicon_count(sent) for sent in sentences]\n",
    "    return statistics.median(word_cnt)\n",
    "\n",
    "def fkl(text):\n",
    "    \"\"\"Compute Flesch-Kincaid grade level readability measure\"\"\"\n",
    "\n",
    "    return textstat.flesch_kincaid_grade(text)\n",
    "\n",
    "def mtld(text):\n",
    "    \"\"\"Measure of textual lexical diversity\"\"\"\n",
    "\n",
    "    lex = LexicalRichness(text)\n",
    "    return lex.mtld()\n",
    "\n",
    "#-----Laughter Frequency-----\n",
    "def laughs(text):\n",
    "    \"\"\"Number of laughs\"\"\"\n",
    "\n",
    "    return (len(re.findall('(Laughter)', text)))\n",
    "\n",
    "#-----Pronominal measures-----\n",
    "def pm_i(text):\n",
    "    \"\"\"Number of times 'I' was used\"\"\"\n",
    "\n",
    "    words = text.split()\n",
    "    cnt = len([word for word in words if word=='I'])\n",
    "    return cnt\n",
    "\n",
    "def pm_we(text):\n",
    "    \"\"\"Number of times 'we' was used\"\"\"\n",
    "\n",
    "    text = text.lower()\n",
    "    cnt = len(re.findall('we[.|(]*', text))\n",
    "    return cnt\n",
    "\n",
    "def pm_you(text):\n",
    "    \"\"\"Number of times 'you' was used\"\"\"\n",
    "\n",
    "    text = text.lower()\n",
    "    cnt = len(re.findall('you[.|(]*', text))\n",
    "    return cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ted['mws'] = ted['transcript'].apply(mws)\n",
    "ted['fkl'] = ted['transcript'].apply(fkl)\n",
    "ted['mtld'] = ted['transcript'].apply(mtld)\n",
    "\n",
    "ted['cnt_laughs'] = ted['transcript'].apply(laughs)\n",
    "ted['cnt_i'] = ted['transcript'].apply(pm_i)\n",
    "ted['cnt_we'] = ted['transcript'].apply(pm_we)\n",
    "ted['cnt_you'] = ted['transcript'].apply(pm_you)\n",
    "\n",
    "ted['lf'] = round(ted.cnt_laughs / ted.duration, 2)\n",
    "ted['pm_i'] = round(ted.cnt_i / ted.duration, 2)\n",
    "ted['pm_we'] = round(ted.cnt_we / ted.duration, 2) \n",
    "ted['pm_you'] = round(ted.cnt_you / ted.duration, 2)\n",
    "\n",
    "ted['spm'] = round(ted.syll / ted.duration / 60, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ted.head()"
   ]
  },
  {
   "source": [
    "### Compute NIP measure using Named Entity Recognition from spaCy"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run python -m spacy download en_core_web_lg\n",
    "# store the path of this into `path_to_trained_spacy_pipeline`\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(path_to_trained_spacy_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nip_len(x):\n",
    "\n",
    "    # convert to document\n",
    "    doc = nlp(x)\n",
    "\n",
    "    # NER dictionary\n",
    "    ner_dict = defaultdict(list)\n",
    "    for ent in doc.ents:\n",
    "        ner_dict[ent.label_].append(ent.text)\n",
    "\n",
    "    # aggregate components\n",
    "    percent = ner_dict['PERCENT']\n",
    "    quant = ner_dict['QUANTITY']\n",
    "    ordinal = ner_dict['ORDINAL']\n",
    "    cardinal = ner_dict['CARDINAL']\n",
    "\n",
    "    return (len(percent + quant + ordinal + cardinal))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute nip\n",
    "ted['nip_comps_cnt'] = ted['transcript'].apply(get_nip_len)\n",
    "ted['nip'] = round(ted['nip_comps_cnt'] / ted['duration'], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data with the newly computed measures\n",
    "ted[['talk_name', 'event', 'event_cat', 'views', 'tags', 'p_year', 'transcript', 'mws',\n",
    "    'fkl', 'mtld', 'cnt_laughs', 'cnt_i', 'cnt_we', 'cnt_you', 'lf', 'pm_i', 'pm_we', 'pm_you',\n",
    "    'nip_comps_cnt', 'nip']].to_csv('../data/processed/ted_measures.csv', index=False)"
   ]
  }
 ]
}